{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "LXjq2qaWA07q",
    "outputId": "4e4db250-bca6-41d9-bae4-5b4f0a038327"
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#make the self written stuff findeable independent on who downloads it\n",
    "#sts.path.append( ) makes the module importable\n",
    "# os.path.dirname(os.getcwd()) provides the parent directory\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory is : C:\\Users\\Utis\\Documents\\GitHub\\tensorflow_1.X_playground\\regression\\notebooks\n",
      "Directory name is : notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "dirpath = os.getcwd()\n",
    "print(\"current directory is : \" + dirpath)\n",
    "foldername = os.path.basename(dirpath)\n",
    "print(\"Directory name is : \" + foldername)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.generators import ToyDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mT7mnXzlA3dR"
   },
   "outputs": [],
   "source": [
    "from helpers import _TFColor, colors, lazy_property\n",
    "import helpers\n",
    "TFColor = _TFColor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ov0Vd24oOWy4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SWHoPlDA5wz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
    "import matplotlib.pyplot as plt; plt.style.use(matplotlib_style)\n",
    "import matplotlib.axes as axes;\n",
    "from matplotlib.patches import Ellipse\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set_context('notebook')\n",
    "from IPython.core.pylabtools import figsize\n",
    "#@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)\n",
    "notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf']\n",
    "%config InlineBackend.figure_format = notebook_screen_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A72dk5dHCuth"
   },
   "outputs": [],
   "source": [
    "figsize(20,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2BacnSpXA9jt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Tensorflow version 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    " \n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    " \n",
    "logging.set_verbosity(logging.INFO)\n",
    "logging.log(logging.INFO, \"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.generators import ToyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SEQ_LEN = 300#240\n",
    "\n",
    "\n",
    "dataset = ToyDataset(DATA_SEQ_LEN,DATA_SEQ_LEN)\n",
    "X, y = dataset.X, dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMZ3HgTjBAJ0"
   },
   "outputs": [],
   "source": [
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZtXit0Bu44C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9zRpJeLBCH-"
   },
   "outputs": [],
   "source": [
    "gdef = dataset.graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CrKlrRKtBRtW"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_tgZ-c3qniD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v4nk2_pnqncw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JjWJoCTFIfl"
   },
   "source": [
    "Now the shufflebuffer includes the whole dataset and we have loads of randomness in the sequence to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZWSyX6bFH0-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dY1V3xLa0Z5"
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     X_meh, y_meh = sess.run(it.get_next())\n",
    "    \n",
    "# plt.plot(X_meh,y_meh,'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M30iTsViRI28"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyKJvYhDbEZj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nC_N3GTnReay"
   },
   "outputs": [],
   "source": [
    "# with tf.Graph().as_default() as g_1:\n",
    "#   dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "#   dataset = dataset.repeat()\n",
    "#   dataset = dataset.shuffle(300)\n",
    "#   dataset = dataset.batch(10)\n",
    "#   it = dataset.make_one_shot_iterator()\n",
    "#   get_next = tf.identity(it.get_next(), name = 'next')\n",
    "#   X_ =  tf.identity(it.get_next()[0], name=\"X\")\n",
    "#   y_ =  tf.identity(it.get_next()[1], name=\"y\")\n",
    "\n",
    "#   gdef = g_1.as_graph_def()\n",
    "  \n",
    "#   #g_1.get_tensor_by_name('X:0')\n",
    "#   #g_1.get_tensor_by_name(\"y:0\")\n",
    "\n",
    "#   #it = dataset.make_one_shot_iterator()\n",
    "\n",
    "# #X,y = it.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eci6fdYxReWi"
   },
   "outputs": [],
   "source": [
    "# g2 = tf.Graph()\n",
    "\n",
    "# with g2.as_default():\n",
    "#   dataset_graph = tf.import_graph_def(gdef, return_elements = ['X:0',\"y:0\", 'next'])\n",
    "#   next_ = g2.get_tensor_by_name('import/next:0')\n",
    "#   #X_ = g2.get_tensor_by_name('import/X:0')\n",
    "#   #y_ = g2.get_tensor_by_name('import/y:0')\n",
    "  \n",
    "#   with tf.Session() as sess:\n",
    "#     X_, y_ = sess.run(next_)\n",
    "#     #X_restored = sess.run(X_)\n",
    "#     #y_restored = sess.run(y_)\n",
    "#     plt.plot(X_,y_,'*')\n",
    "#     print(len(X_))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     X_, y_ = sess.run(next_)\n",
    "#     #X_restored = sess.run(X_)\n",
    "#     #y_restored = sess.run(y_)\n",
    "#     plt.plot(X_,y_,'*')\n",
    "#     print(len(X_))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     X_, y_ = sess.run(next_)\n",
    "#     #X_restored = sess.run(X_)\n",
    "#     #y_restored = sess.run(y_)\n",
    "#     plt.plot(X_,y_,'*')\n",
    "#     print(len(X_))\n",
    "#     #plt.plot(y_,'*')\n",
    "#     #plt.plot(y_restored)\n",
    "#     #plt.plot(X_restored)\n",
    "#     #plt.plot(sess.run(y_))\n",
    "#     #plt.plot(X_restored)\n",
    "\n",
    "#   print(type(dataset_graph))\n",
    "#   print(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2eeWjRUhZUH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tP0qfAVyGTd7"
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/33748552/tensorflow-how-to-replace-a-node-in-a-calculation-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdpIQ8mOGTbt"
   },
   "outputs": [],
   "source": [
    "#from models.networks import EnsembleNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yg4rF6-SGTZQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class EnsembleNetwork(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            ds_graph,\n",
    "            num_neurons=[10, 10, 10],\n",
    "            num_features=1,\n",
    "            learning_rate=0.001,\n",
    "            activations=None,  #[tf.nn.tanh,tf.nn.relu,tf.sigmoid]\n",
    "            dropout_layers=None,  #[True,False,True]\n",
    "            initialisation_scheme=None,  #[tf.random_normal,tf.random_normal,tf.random_normal]\n",
    "            optimizer=None,  #defaults to GradiendDescentOptimizer,\n",
    "            num_epochs=None,  #defaults to 1,\n",
    "            seed=None,\n",
    "            #adversarial=None\n",
    "    ):\n",
    "\n",
    "        #necessary parameters\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_layers = len(num_neurons)\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ds_graph = ds_graph\n",
    "        #self.adversarial = adversarial or ADVERSARIAL\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        #optional parameters\n",
    "        self.optimizer = optimizer or tf.train.GradientDescentOptimizer\n",
    "        self.activations = activations or [tf.nn.tanh] * self.num_layers\n",
    "        self.initialisation_scheme = initialisation_scheme or tf.random_normal\n",
    "        self.num_epochs = num_epochs or 1000\n",
    "        self.seed = seed or None\n",
    "        self.initialise_graph\n",
    "        self.initialise_session\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def initialise_graph(self):\n",
    "        #initialise graph\n",
    "        self.g = tf.Graph()\n",
    "        #build graph with self.graph as default so nodes get appended\n",
    "        with self.g.as_default():\n",
    "            dataset_graph = tf.import_graph_def(self.ds_graph, return_elements = ['X:0',\"y:0\"])\n",
    "            self.next = self.g.get_tensor_by_name('import/next:0')\n",
    "            #self.X = tf.cast(self.g.get_tensor_by_name('import/X:0'), dtype=tf.float32)\n",
    "            #self.y = tf.cast(self.g.get_tensor_by_name('import/y:0'), dtype=tf.float32)\n",
    "            self.init_network\n",
    "            self.predict_graph\n",
    "            self.error_graph\n",
    "            self.train_graph\n",
    "            self.init = tf.global_variables_initializer()\n",
    "\n",
    "    @lazy_property\n",
    "    def initialise_session(self):\n",
    "        #initialise session\n",
    "        self.session = tf.Session(graph=self.g)\n",
    "        #initialise global variables\n",
    "        self.session.run(self.init)\n",
    "\n",
    "    @lazy_property\n",
    "    def init_network(self):\n",
    "        if self.seed:\n",
    "            tf.set_random_seed(self.seed)\n",
    "        #inputs\n",
    "        #self.X = self.x_in#batch[0]#self.X#tf.placeholder(tf.float32, (None, self.num_features))\n",
    "        #self.y = self.y_in#batch[1]#y#tf.placeholder(tf.float32, (None, 1))  #regression = 1\n",
    "        self.X = self.next[0]\n",
    "        self.y = self.next[1]\n",
    "\n",
    "        #lists for storage\n",
    "        self.w_list = []\n",
    "        self.b_list = []\n",
    "\n",
    "        #add input x first weights\n",
    "        self.w_list.append(\n",
    "            tf.Variable(\n",
    "                self.initialisation_scheme(\n",
    "                    [self.num_features, self.num_neurons[0]]),\n",
    "                name='w_0'))  #first Matrix\n",
    "        self.b_list.append(\n",
    "            tf.Variable(\n",
    "                tf.ones(shape=[self.num_features]), name='b_0' ))\n",
    "        #for each layer over 0 add a n x m matrix and a bias term\n",
    "        for i, num_neuron in enumerate(self.num_neurons[1:]):\n",
    "            n_inputs = self.num_neurons[i]  #for first hidden layer 3\n",
    "            n_outputs = self.num_neurons[i + 1]  #for first hidden layer 5\n",
    "\n",
    "            self.w_list.append(\n",
    "                tf.Variable(\n",
    "                    self.initialisation_scheme([n_inputs, n_outputs]),\n",
    "                    name='w_' + str(i)))\n",
    "            self.b_list.append(\n",
    "                tf.Variable(tf.ones(shape=[n_inputs]), name='b_' + str(i)))\n",
    "\n",
    "        #add last layer m  x 1 for output\n",
    "        self.w_list.append(\n",
    "            tf.Variable(\n",
    "                self.initialisation_scheme([self.num_neurons[-1], 1]),\n",
    "                name='w_-1'))  #this is a regression\n",
    "        self.b_list.append(\n",
    "            tf.Variable(\n",
    "                tf.ones(shape=[self.num_neurons[-1]]), name='b_' + str(\n",
    "                    len(self.num_neurons) + 1)))\n",
    "\n",
    "    @lazy_property\n",
    "    def predict_graph(self):\n",
    "        #set layer_input to input\n",
    "        layer_input = self.X\n",
    "\n",
    "        #for each layer do\n",
    "        for i, w in enumerate(self.w_list):\n",
    "\n",
    "            #z = input x Weights\n",
    "            a = tf.matmul(layer_input, w, name='matmul_' + str(i))\n",
    "\n",
    "            #z + bias\n",
    "            if i <= self.num_layers:\n",
    "                bias = self.b_list[i]\n",
    "                a = tf.add(a, bias)\n",
    "\n",
    "            #a = sigma(z) if not last layer and regression\n",
    "            if i < self.num_layers:\n",
    "\n",
    "                a = self.activations[i](a)\n",
    "            #set layer input to a for next cycle\n",
    "            layer_input = a\n",
    "\n",
    "        return a\n",
    "\n",
    "    @lazy_property\n",
    "    def error_graph(self):\n",
    "\n",
    "        #y_hat is a // output of prediction graph\n",
    "        y_hat = self.predict_graph\n",
    "\n",
    "        #error is mean squared error of placehilder y and prediction\n",
    "        error = tf.losses.mean_squared_error(self.y, y_hat)  #tf.square(\n",
    "        #self.y - y_hat)  #\n",
    "\n",
    "        return error\n",
    "\n",
    "    @lazy_property\n",
    "    def train_graph(self):\n",
    "\n",
    "        #error is the error from error graph\n",
    "        error = self.error_graph\n",
    "\n",
    "        #optimizer is self.optimizer\n",
    "        optimizer = self.optimizer(learning_rate=self.learning_rate)\n",
    "        \n",
    "        \n",
    "        return optimizer.minimize(error)\n",
    "      \n",
    "      \n",
    "    def fit(self,num_epochs=10):\n",
    "      error_list = []\n",
    "      for i in range(num_epochs):\n",
    "        error_list.append(self.session.run(self.train_graph))\n",
    "        \n",
    "      return error_list\n",
    "\n",
    "      \n",
    "    def fit_dataset(self,X,y):\n",
    "      self.X = X\n",
    "      self.y = y\n",
    "      self.session.run(self.train_graph)\n",
    "      \n",
    "    def fit_generator(self,generator):\n",
    "      self.session.run(self.train_graph, generator)\n",
    "\n",
    "    def train_offline(self, X, y):\n",
    "        for epoch in range(self.num_epochs):\n",
    "\n",
    "            self.session.run(self.train_graph,\n",
    "                             feed_dict={self.X: X,\n",
    "                                        self.y: y})\n",
    "\n",
    "    def train(self, X, y, shuffle=True, online=False):\n",
    "        #print('X is {}'.format(X[:10]))\n",
    "        if online:\n",
    "            for epoch in range(self.num_epochs):\n",
    "                self.train_one_epoch(X, y, shuffle)\n",
    "        else:\n",
    "            self.train_offline(X, y)\n",
    "\n",
    "    def train_one_epoch(self, epoch_X, epoch_y, shuffle=True):\n",
    "        if shuffle == True:\n",
    "            epoch_X, epoch_y, _ = unison_shuffled_copies(\n",
    "                np.squeeze(epoch_X), np.squeeze(epoch_y), True)\n",
    "        #print(epoch_X[:10])\n",
    "\n",
    "        for batch_X, batch_y in zip(epoch_X, epoch_y):\n",
    "            self.train_one(batch_X, batch_y)\n",
    "            #if self.adversarial:\n",
    "            #    self.train_one(batch_X+0.05, batch_y)\n",
    "            #    self.train_one(batch_X-0.05, batch_y)\n",
    "\n",
    "    def train_one(self, batch_X, batch_y):\n",
    "        batch_X = np.expand_dims(batch_X, 1)\n",
    "        batch_y = np.expand_dims(batch_y, 1)\n",
    "        self.session.run(self.train_graph,\n",
    "                         feed_dict={self.X: batch_X,\n",
    "                                    self.y: batch_y})\n",
    "\n",
    "    def train_and_evaluate(self, X, y, shuffle=False):\n",
    "        errors = []\n",
    "        for epoch in range(self.num_epochs):\n",
    "\n",
    "            #self.session.run(self.train_graph,\n",
    "            #                 feed_dict={self.X: X,\n",
    "            #                            self.y: y})\n",
    "            self.train_one_epoch(X, y, shuffle)\n",
    "\n",
    "                    \n",
    "\n",
    "            errors += list(np.sqrt((y - self.predict(X))**2))\n",
    "\n",
    "            #self.session.run(self.predict_graph,\n",
    "            #                          feed_dict={self.X: X,\n",
    "            #                                     self.y: y}))**2))\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def predict(self, X):\n",
    "        #return self.session.run(self.predict_graph)\n",
    "        return self.session.run(self.predict_graph, feed_dict={self.X: X})\n",
    "      \n",
    "    def predict_meh(self):\n",
    "      \"\"\"never use this it's stupid\"\"\"\n",
    "      return self.session.run(self.predict_graph) \n",
    "\n",
    "    def kill(self):\n",
    "        self.session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQwOTNJl37JZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EnsembleNetwork(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            ds_graph,\n",
    "            num_neurons=[10, 5, 3],\n",
    "            num_features=1,\n",
    "            learning_rate=0.001,\n",
    "            activations=None,  #[tf.nn.tanh,tf.nn.relu,tf.sigmoid]\n",
    "            dropout_layers=None,  #[True,False,True]\n",
    "            initialisation_scheme=None,  #[tf.random_normal,tf.random_normal,tf.random_normal]\n",
    "            optimizer=None,  #defaults to GradiendDescentOptimizer,\n",
    "            num_epochs=None,  #defaults to 1,\n",
    "            seed=None,\n",
    "            adversarial=None,\n",
    "            initialisation_params=None,\n",
    "            l2=None,\n",
    "            l=None):\n",
    "\n",
    "        #necessary parameters\n",
    "        self.num_neurons = num_neurons\n",
    "        self.ds_graph = ds_graph\n",
    "\n",
    "        self.num_layers = len(num_neurons)\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate or 0.001\n",
    "        self.adversarial = adversarial or False\n",
    "        self.initialisation_params = initialisation_params or {}\n",
    "        self.l2 = l2 or False\n",
    "        self.l = l or 0.05\n",
    "\n",
    "        #optional parameters\n",
    "        self.optimizer = optimizer or tf.train.AdamOptimizer  #tf.train.GradientDescentOptimizer\n",
    "        self.activations = activations or [tf.nn.relu  #tf.nn.tanh\n",
    "                                           ] * self.num_layers  #tanh,relu, \n",
    "        self.initialisation_scheme = initialisation_scheme or tf.keras.initializers.he_normal  ##tf.contrib.layers.xavier_initializer  #\n",
    "        #tf.contrib.layers.xavier_initializer  #tf.truncated_normal  #tf.random_uniform#\n",
    "        self.num_epochs = num_epochs or meta_num_epochs\n",
    "        self.seed = seed or None\n",
    "\n",
    "        self.initialise_graph\n",
    "        self.initialise_session\n",
    "        print('initialising Network {}'.format(type(self)))\n",
    "        \n",
    "    \n",
    "\n",
    "    @lazy_property\n",
    "    def initialise_graph(self):\n",
    "        #initialise graph\n",
    "        self.g = tf.Graph()\n",
    "        #build graph with self.graph as default so nodes get appended\n",
    "        with self.g.as_default():\n",
    "            dataset_graph = tf.import_graph_def(self.ds_graph, return_elements = ['X:0',\"y:0\"])\n",
    "            self.next = self.g.get_tensor_by_name('import/next:0')\n",
    "            self.init_network\n",
    "            self.predict_graph\n",
    "            self.error_graph\n",
    "            self.train_graph\n",
    "            self.init = tf.global_variables_initializer()\n",
    "\n",
    "    @lazy_property\n",
    "    def initialise_session(self):\n",
    "        #initialise session\n",
    "        self.session = tf.Session(graph=self.g)\n",
    "        #initialise global variables\n",
    "        self.session.run(self.init)\n",
    "\n",
    "    @lazy_property\n",
    "    def init_network(self):\n",
    "        if self.seed:\n",
    "            tf.set_random_seed(self.seed)\n",
    "        #inputs\n",
    "        self.X = self.next[0]\n",
    "        self.y = self.next[1]\n",
    "        #lists for storage\n",
    "        self.w_list = []\n",
    "        self.b_list = []\n",
    "\n",
    "        #add input x first weights\n",
    "        #        self.w_list.append(\n",
    "        #            tf.Variable(\n",
    "        #                self.initialisation_scheme(\n",
    "        #                    [self.num_features, self.num_neurons[0]]),\n",
    "        #                name='w_0'))  #first Matrix\n",
    "        initialiser = self.initialisation_scheme(seed=self.seed,\n",
    "                                                 **self.initialisation_params)\n",
    "        self.w_list.append(\n",
    "            tf.Variable(\n",
    "                initialiser([self.num_features, self.num_neurons[0]]),\n",
    "                name='w_0'))\n",
    "\n",
    "        #for each layer over 0 add a n x m matrix and a bias term\n",
    "        for i, num_neuron in enumerate(self.num_neurons[1:]):\n",
    "            n_inputs = self.num_neurons[i]  #for first hidden layer 3\n",
    "            n_outputs = self.num_neurons[i + 1]  #for first hidden layer 5\n",
    "\n",
    "            self.w_list.append(\n",
    "                tf.Variable(\n",
    "                    initialiser([n_inputs, n_outputs]), name='w_' + str(i)))\n",
    "            self.b_list.append(\n",
    "                tf.Variable(tf.ones(shape=[n_inputs]), name='b_' + str(i)))\n",
    "\n",
    "        #add last layer m  x 1 for output\n",
    "        self.w_list.append(\n",
    "            tf.Variable(initialiser([self.num_neurons[-1], 1]),\n",
    "                        name='w_-1'))  #this is a regression\n",
    "        self.b_list.append(\n",
    "            tf.Variable(\n",
    "                tf.ones(shape=[self.num_neurons[-1]]), name='b_' + str(\n",
    "                    len(self.num_neurons) + 1)))\n",
    "\n",
    "    @lazy_property\n",
    "    def predict_graph(self):\n",
    "        #set layer_input to input\n",
    "        layer_input = self.X\n",
    "\n",
    "        #for each layer do\n",
    "        for i, w in enumerate(self.w_list):\n",
    "\n",
    "            #z = input x Weights\n",
    "            a = tf.matmul(layer_input, w, name='matmul_' + str(i))\n",
    "\n",
    "            #z + bias\n",
    "            if i < self.num_layers:\n",
    "            #if i > 0:\n",
    "              bias = self.b_list[i]\n",
    "              a = tf.add(a, bias)\n",
    "\n",
    "            #a = sigma(z) if not last layer and regression\n",
    "            if i < self.num_layers:\n",
    "\n",
    "                a = self.activations[i](a)\n",
    "            #set layer input to a for next cycle\n",
    "            layer_input = a\n",
    "\n",
    "        return a\n",
    "\n",
    "    @lazy_property\n",
    "    def error_graph(self):\n",
    "\n",
    "        #y_hat is a // output of prediction graph\n",
    "        y_hat = self.predict_graph\n",
    "\n",
    "        #error is mean squared error of placehilder y and prediction\n",
    "        error = tf.losses.mean_squared_error(self.y, y_hat)  #tf.square(\n",
    "        #self.y - y_hat)  #\n",
    "\n",
    "        if self.l2:\n",
    "            # Loss function with L2 Regularization with beta=0.01\n",
    "            regularizers = tf.reduce_sum(\n",
    "                [tf.nn.l2_loss(weights) for weights in self.w_list])\n",
    "            error = tf.reduce_mean(error + self.l * regularizers)\n",
    "\n",
    "        return error\n",
    "\n",
    "    @lazy_property\n",
    "    def train_graph(self):\n",
    "\n",
    "        #error is the error from error graph\n",
    "        error = self.error_graph\n",
    "\n",
    "        #optimizer is self.optimizer\n",
    "        optimizer = self.optimizer(learning_rate=self.learning_rate)\n",
    "\n",
    "        return optimizer.minimize(error)\n",
    "\n",
    "    def train_offline(self, X, y):\n",
    "        for epoch in range(self.num_epochs):\n",
    "\n",
    "            self.session.run(self.train_graph,\n",
    "                             feed_dict={self.X: X,\n",
    "                                        self.y: y})\n",
    "\n",
    "    def fit(self, X, y, shuffle=True, online=True):\n",
    "        #print('X is {}'.format(X[:10]))\n",
    "\n",
    "        #if shuffle:\n",
    "\n",
    "        if online:\n",
    "            for epoch in range(self.num_epochs):\n",
    "                if shuffle:\n",
    "                    X, y, _ = self.shuffle_data(X, y)\n",
    "                X = self.check_input_dimensions(X)\n",
    "                y = self.check_input_dimensions(y)\n",
    "                self.train_one_epoch(X, y, shuffle)\n",
    "        else:\n",
    "            if shuffle:\n",
    "                X, y, _ = self.shuffle_data(X, y)\n",
    "            X = self.check_input_dimensions(X)\n",
    "            y = self.check_input_dimensions(y)\n",
    "            self.train_offline(X, y)\n",
    "            \n",
    "            \n",
    "    def fit(self,epochs):\n",
    "      for epoch in range(epochs):\n",
    "        self.session.run(self.train_graph)\n",
    "        \n",
    "\n",
    "    def train_one_epoch(self, epoch_X, epoch_y, shuffle=True):\n",
    "\n",
    "        if shuffle == True:\n",
    "            epoch_X, epoch_y, _ = self.shuffle_data(\n",
    "                #np.squeeze(epoch_X), np.squeeze(epoch_y))\n",
    "                epoch_X,\n",
    "                epoch_y)\n",
    "        #print(epoch_X[:10])\n",
    "        epoch_X = self.check_input_dimensions(epoch_X)\n",
    "        epoch_y = self.check_input_dimensions(epoch_y)\n",
    "        for batch_X, batch_y in zip(epoch_X, epoch_y):\n",
    "            self.train_one(batch_X, batch_y)\n",
    "            if self.adversarial:\n",
    "                self.train_one(batch_X + 0.05, batch_y)\n",
    "                self.train_one(batch_X - 0.05, batch_y)\n",
    "\n",
    "    def train_one(self, batch_X, batch_y):\n",
    "        batch_X = self.check_input_dimensions(batch_X)\n",
    "        batch_y = self.check_input_dimensions(batch_y)\n",
    "        batch_X = np.array(batch_X).T\n",
    "\n",
    "        batch_X, _, batch_y, _ = train_test_split(\n",
    "            batch_X, batch_y, test_size=0.00, random_state=self.seed)\n",
    "\n",
    "        #print('what')\n",
    "        #print('X size: {}'.format(batch_X.shape))\n",
    "        #print('y size: {}'.format(batch_y.shape))\n",
    "        self.session.run(self.train_graph,\n",
    "                         feed_dict={self.X: batch_X,\n",
    "                                    self.y: batch_y})\n",
    "\n",
    "    def train_and_evaluate(self, X, y, shuffle=False):\n",
    "        errors = []\n",
    "        for epoch in range(self.num_epochs):\n",
    "\n",
    "            #self.session.run(self.train_graph,\n",
    "            #                 feed_dict={self.X: X,\n",
    "            #                            self.y: y})\n",
    "            self.train_one_epoch(X, y, shuffle)\n",
    "\n",
    "            errors += list(np.sqrt((y - self.predict(X))**2))\n",
    "\n",
    "            #self.session.run(self.predict_graph,\n",
    "            #                          feed_dict={self.X: X,\n",
    "            #                                     self.y: y}))**2))\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.check_input_dimensions(X)\n",
    "\n",
    "        return self.session.run(self.predict_graph,\n",
    "                                feed_dict={self.X: X}).squeeze()\n",
    "\n",
    "    def kill(self):\n",
    "        self.session.close()\n",
    "\n",
    "    def check_input_dimensions(self, array):\n",
    "        \"\"\"Makes sure arrays are compatible with Tensorflow input\n",
    "        can't have array.shape = (X,),\n",
    "        needs to be array.shape = (X,1)\"\"\"\n",
    "        #y = array\n",
    "        #y = np.reshape(y, [y.shape[0], 1])\n",
    "        #return y\n",
    "        if len(array.shape) <= 1:\n",
    "\n",
    "            return np.expand_dims(array, 1)\n",
    "        else:\n",
    "\n",
    "            return array\n",
    "\n",
    "    def shuffle_data(self, a, b):\n",
    "        assert len(a) == len(b)\n",
    "        p = np.random.permutation(len(a))\n",
    "        sorted_index = np.argsort(p)\n",
    "        p = np.squeeze(p)\n",
    "        return a[p], b[p], sorted_index\n",
    "\n",
    "    def network_mutli_dimensional_scatterplot(self, X_test, y_test, X=None,\n",
    "                                              y=None, figsize=(20, 50),\n",
    "                                              filename=None):\n",
    "\n",
    "        #y_hat = self.predict(X_test)\n",
    "        #print(pred_dict)\n",
    "        #std = self.predict_var(X_test)\n",
    "        y_hat, std = self.get_prediction_and_std(X_test)\n",
    "\n",
    "        #plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        #plt.scatter(X[:,5],y)\n",
    "\n",
    "        num_features = len(X_test.T)\n",
    "        for i, feature in enumerate(X_test.T):\n",
    "            #sort the arrays\n",
    "            s = np.argsort(feature)\n",
    "            var = y_hat[s] - std[s]\n",
    "            var2 = y_hat[s] + std[s]\n",
    "            print(feature.shape)\n",
    "            print(var.shape)\n",
    "\n",
    "            plt.subplot(num_features, 1, i + 1)\n",
    "            plt.plot(\n",
    "                feature[s],\n",
    "                y_hat[s],\n",
    "                label='predictive Mean', )\n",
    "            plt.fill_between(feature[s].ravel(), y_hat[s].ravel(),\n",
    "                             var.ravel(), alpha=.3, color='b',\n",
    "                             label='uncertainty')\n",
    "            plt.fill_between(feature[s].ravel(), y_hat[s].ravel(),\n",
    "                             var2.ravel(), alpha=.3, color='b')\n",
    "            plt.scatter(feature[s], y_test[s], label='data', s=20,\n",
    "                        edgecolor=\"black\", c=\"darkorange\")\n",
    "            plt.xlabel(\"data\")\n",
    "            plt.ylabel(\"target\")\n",
    "            plt.title(\"Ensemble\")\n",
    "            plt.legend()\n",
    "            if filename is not None:\n",
    "                plt.savefig(filename)\n",
    "\n",
    "        if filename is not None:\n",
    "            plt.savefig(filename)\n",
    "        #plt.show()\n",
    "        return fig\n",
    "\n",
    "    def compute_rsme(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        #y_hat = pred_dict['means']\n",
    "        #std = pred_dict['stds']        #print(y_hat.shape,std.shape,y.shape)\n",
    "\n",
    "        return np.sqrt(np.mean((y_hat - y)**2))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.compute_rsme(X, y)\n",
    "\n",
    "    def compute_error_vec(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        return y - y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b01Pmu6VGaxn"
   },
   "outputs": [],
   "source": [
    "\n",
    "class EnsembleParent(object):\n",
    "    \"\"\"\n",
    "    https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/05_Ensemble_Learning.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimator_stats=None, num_epochs=10):\n",
    "        pass\n",
    "\n",
    "    def train(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X, return_samples=False):\n",
    "        pass\n",
    "\n",
    "    def plot(self, X, y, plot_samples=False, original_func=None,\n",
    "             sorted_index=None):\n",
    "        preds_dict = self.predict(X, True)\n",
    "        y_hats = np.array([mean[0] for mean in preds_dict['means']])\n",
    "        y_stds = np.array([std[0] for std in preds_dict['stds']])\n",
    "        samples = preds_dict['samples']\n",
    "        X_plot = [x[0] for x in np.array(X)]  #[sorted_index]]\n",
    "\n",
    "        #print(y_hats.shape)\n",
    "        #print(y_stds.shape)\n",
    "        #print(y.shape)\n",
    "        #print(np.array(X).shape)\n",
    "        #X_plot = X_plot[sorted_index]\n",
    "        y = np.array(y)  #[sorted_index]\n",
    "        y_hats = y_hats  #[sorted_index]\n",
    "        y_stds = y_stds  #[sorted_index]\n",
    "\n",
    "        plt.plot(X, y, 'k*', label='Data', alpha=0.4)\n",
    "\n",
    "        plt.plot(X_plot, y_hats, label='predictive mean')\n",
    "        plt.fill_between(X_plot, y_hats + y_stds, y_hats, alpha=.3, color='b')\n",
    "        plt.fill_between(X_plot, y_hats - y_stds, y_hats, alpha=.3, color='b')\n",
    "\n",
    "        if plot_samples:\n",
    "            for i, sample in enumerate(samples):\n",
    "\n",
    "                plt.plot(X_plot, sample, label='sample {}'.format(i),\n",
    "                         alpha=max(1 / len(samples), 0.3))\n",
    "        if original_func:\n",
    "            y_original = original_func(X_plot)\n",
    "            plt.plot(X, y_original, label='generating model')\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "class VanillaEnsemble(EnsembleParent):\n",
    "    \"\"\"\n",
    "    https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/05_Ensemble_Learning.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,ds_graph = None, estimator_stats=None, num_epochs=10):\n",
    "\n",
    "        default_ensemble = [{\n",
    "            'ds_graph' : gdef,\n",
    "            'num_neurons': [10, 50, 20],\n",
    "            'num_epochs': num_epochs,\n",
    "            'seed':42\n",
    "        }, {\n",
    "            'ds_graph' : gdef,\n",
    "            'num_neurons': [100, 10],\n",
    "            'num_epochs': num_epochs,\n",
    "            'seed': 43\n",
    "        }, {\n",
    "            'ds_graph' : gdef,\n",
    "            'num_neurons': [5, 150],\n",
    "            'num_epochs': num_epochs,\n",
    "            'seed': 44\n",
    "        }]\n",
    "\n",
    "        self.estimator_stats = estimator_stats or default_ensemble\n",
    "        self.estimator_list = [\n",
    "            EnsembleNetwork(**x) for x in self.estimator_stats\n",
    "        ]\n",
    "\n",
    "    def fit(self, epochs = 10):\n",
    "        '''This is where we build in the Online Bootstrap'''\n",
    "        for estimator in self.estimator_list:\n",
    "            #estimator.train_and_evaluate(X, y,shuffle=False)\n",
    "            estimator.fit(epochs)#(X,y)\n",
    "        #system('say training  complete')\n",
    "        \n",
    "    def train(self,X,y,epochs = 10):\n",
    "      for epoch in epochs:\n",
    "        for estimator in self.estimator_list:\n",
    "          estimator.train(X,y)\n",
    "\n",
    "    def predict(self, X, return_samples=False):\n",
    "        pred_list = []\n",
    "        for estimator in self.estimator_list:\n",
    "            prediction = estimator.predict(X)\n",
    "            pred_list.append(prediction)\n",
    "        #for i,sample in enumerate(pred_list):\n",
    "        #   assert(np.isnan(sample) is False), 'sample {} contains NaN'.format(i)\n",
    "        stds = np.std(pred_list, 0)\n",
    "        means = np.mean(pred_list, 0)\n",
    "        #assert(np.isnan(stds) is False)\n",
    "        #assert(np.isnan(means) is False)\n",
    "        return_dict = {'stds': stds, 'means': means}\n",
    "        if return_samples:\n",
    "            return_dict['samples'] = pred_list\n",
    "        #system('say prediction complete')\n",
    "\n",
    "        return return_dict\n",
    "\n",
    "\n",
    "class OnlineBootstrapEnsemble(VanillaEnsemble):\n",
    "    \"\"\"\n",
    "    https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/05_Ensemble_Learning.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self,gdef, estimator_stats = None,num_estimators=10,num_epochs=10,seed=10):\n",
    "        \n",
    "        default_ensemble = [{\n",
    "            'ds_graph' : gdef,\n",
    "            'num_neurons': [10, 5, 5, 2],\n",
    "            'num_epochs': num_epochs,\n",
    "            'seed':42\n",
    "        },\n",
    "            {\n",
    "            'ds_graph' : gdef,\n",
    "            'num_neurons': [10, 10, 5],\n",
    "            'num_epochs': num_epochs,\n",
    "            'seed': 43\n",
    "        }, {\n",
    "            'ds_graph' : gdef,\n",
    "            'num_neurons': [5, 15, 5],\n",
    "            'num_epochs': num_epochs,\n",
    "            'seed': 44\n",
    "        }\n",
    "        ]\n",
    "\n",
    "        self.estimator_stats = estimator_stats or default_ensemble\n",
    "        self.estimator_list = [\n",
    "            EnsembleNetwork(**x) for x in self.estimator_stats\n",
    "        ]\n",
    "        \n",
    "\n",
    "    def fit(self, epochs = 10):\n",
    "      for i in range(epochs*2): # only training in half of the cases\n",
    "        for estimator in self.estimator_list:\n",
    "          if np.random.random() > 0.5:\n",
    "            #estimator.train_and_evaluate(X, y,shuffle=False)\n",
    "              estimator.fit(epochs)#(X,y)\n",
    "        #system('say training  complete')\n",
    "\n",
    "    def train(self,X,y,epochs = 10):\n",
    "      for epoch in range(epochs*2):\n",
    "        \n",
    "        for estimator in self.estimator_list:\n",
    "          #mask = np.random.choice([0,1], size= len(y)).astype(bool)\n",
    "          #X = X[mask]\n",
    "          #y = y[mask]\n",
    "          estimator.train(X,y)\n",
    "\n",
    "            \n",
    "    def predict(self,X,return_samples=False):\n",
    "        pred_list = []\n",
    "        for estimator in self.estimator_list:\n",
    "            prediction = estimator.predict(X)\n",
    "            pred_list.append(prediction)\n",
    "            \n",
    "        stds = np.std(pred_list,0)\n",
    "        means = np.mean(pred_list,0)\n",
    "        \n",
    "        \n",
    "        return_dict = {'stds':stds,'means':means}\n",
    "        if return_samples:\n",
    "            return_dict['samples'] = pred_list\n",
    "        #system('say prediction complete')\n",
    "\n",
    "        return return_dict\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mkvkHjo7H3SH"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1okuALpXbRW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGXXHpDcTTu8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gGDQ39GZi4su"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "01rIB_H0h4NQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9QLK6Wbh6pq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x11520 with 0 Axes>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x11520 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_epochs = 20\n",
    "plt.figure(figsize = (16,8*meta_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WD-4krkxTajU"
   },
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OWJu3gWKF_z"
   },
   "outputs": [],
   "source": [
    "# nn = EnsembleNetwork(gdef, learning_rate= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fiSjmZ_3qjhB"
   },
   "outputs": [],
   "source": [
    "# nn.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sbge-yoQq3Gm"
   },
   "outputs": [],
   "source": [
    "# plt.plot(nn.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvD4WvlAq--V"
   },
   "outputs": [],
   "source": [
    "# for i in range(100):nn.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcqUB3w0q_oB"
   },
   "outputs": [],
   "source": [
    "# plt.plot(nn.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQCNYvBRq1-e"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1,2,3,4,5]\n",
    "layers = [\n",
    "    [10, 50, 20, 30, 10],\n",
    "    [100, 50],\n",
    "    [30, 30, 30],\n",
    "    [5, 10, 100],\n",
    "    [50, 50, 50]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_stats = [{\n",
    "            'ds_graph' : gdef,\n",
    "            'num_neurons': num_neurons,\n",
    "            'num_epochs': 10,\n",
    "            'seed':seed,\n",
    "    'activations': [tf.nn.tanh,tf.nn.tanh,tf.nn.tanh,tf.nn.tanh,tf.nn.relu]\n",
    "    #activagtions:tf.nn.relu  #tf.nn.tanh\n",
    "        } for seed, num_neurons in zip(seeds,layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FL0nKOP1w4E5"
   },
   "outputs": [],
   "source": [
    "van = OnlineBootstrapEnsemble(gdef,num_epochs=10,estimator_stats=estimator_stats)\n",
    "#van = VanillaEnsemble(gdef,num_epochs=10,estimator_stats=estimator_stats)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnfjKEyQxMcv"
   },
   "outputs": [],
   "source": [
    "def get_mean_var(pred_dict):\n",
    "    return pred_dict['means'],pred_dict['stds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIb1A-grxOwu"
   },
   "outputs": [],
   "source": [
    "len(van.estimator_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AoNTP-i1xU5j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBC2gn9asNpx"
   },
   "outputs": [],
   "source": [
    "mean, var = get_mean_var(van.predict(X))\n",
    "\n",
    "plt.plot(X,mean,label='X')\n",
    "plt.plot(X,y,'r*')\n",
    "plt.fill_between(np.squeeze(X),np.squeeze(mean), np.squeeze(mean+var),alpha=0.3, color='b')\n",
    "plt.fill_between(np.squeeze(X),np.squeeze(mean), np.squeeze(mean-var),alpha=0.3, color='b')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lHtYSAIuItn"
   },
   "outputs": [],
   "source": [
    "#van.fit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pLpEsbKguMuH"
   },
   "outputs": [],
   "source": [
    "# mean, var = get_mean_var(van.predict(X))\n",
    "\n",
    "# plt.plot(X,mean,label='X')\n",
    "# plt.plot(X,y,'r*')\n",
    "# plt.fill_between(np.squeeze(X),np.squeeze(mean), np.squeeze(mean+var),alpha=0.3, color='b')\n",
    "# plt.fill_between(np.squeeze(X),np.squeeze(mean), np.squeeze(mean-var),alpha=0.3, color='b')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9r3YTVUsIx5B"
   },
   "outputs": [],
   "source": [
    "# a = van.fit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAJQKNKeRla1"
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy \n",
    "\n",
    "\n",
    "def rmse(y_hat, y):\n",
    "    return np.sqrt(np.mean((y_hat - y)**2))\n",
    "\n",
    "\n",
    "def coverage_probability(prediction, variance, truth):\n",
    "    CP = 0\n",
    "    for y_hat, s, y in zip(prediction, variance, truth):\n",
    "        if y_hat + s > y > y_hat - s:\n",
    "            CP += 1\n",
    "\n",
    "    #if (y - 2 * s < 0) and (y + 2 * s > 0):\n",
    "    #    CP += 1\n",
    "    return CP / len(truth)\n",
    "\n",
    "def error_uncertainty_correlation(prediction,variance,truth):\n",
    "    prediction, variance, truth = prediction.flatten(), variance.flatten(), truth.flatten()\n",
    "    assert prediction.shape == truth.shape == variance.shape, 'shapes are: pred {} truth {} variance {}'.format(prediction.shape, truth.shape, variance.shape)\n",
    "    error = (prediction - truth.flatten())**2\n",
    "    correlation = scipy.stats.pearsonr(error.flatten(),variance.flatten())\n",
    "    \n",
    "    #np.correlate(error.flatten(),variance.flatten())\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EHv4MwNCze2c"
   },
   "outputs": [],
   "source": [
    "error_list = list()\n",
    "coverage_list = list()\n",
    "cobeau_list = list()\n",
    "\n",
    "for i in tqdm.tqdm(range(meta_epochs)):\n",
    "\n",
    "\n",
    "  mean, var = get_mean_var(van.predict(X))\n",
    "  plt.figure(figsize = (16,8*10))\n",
    "\n",
    "  plt.subplot(meta_epochs,1,i+1)\n",
    "  plt.plot(X,mean,label='X')\n",
    "  plt.plot(X,y,'r*')\n",
    "  plt.fill_between(np.squeeze(X),np.squeeze(mean), np.squeeze(mean+var),alpha=0.3, color='b')\n",
    "  plt.fill_between(np.squeeze(X),np.squeeze(mean), np.squeeze(mean-var),alpha=0.3, color='b')\n",
    "  plt.legend()\n",
    "\n",
    "\n",
    "  error = rmse(mean,y)\n",
    "  coverage = coverage_probability(mean,var,y)\n",
    "  cobeau = error_uncertainty_correlation(mean,var,y)\n",
    "    \n",
    "  error_list.append(error)\n",
    "  coverage_list.append(coverage)\n",
    "  cobeau_list.append(cobeau)\n",
    "\n",
    "  print('rsme is {}. \\n Coverage is {}. \\n COBEAU is {}.'.format(error,coverage,cobeau))\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  plt.show()\n",
    "  a = van.fit(25)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(error_list,label='error')\n",
    "plt.plot(coverage_list,label='coverage')\n",
    "plt.plot(cobeau_list,label='cobeau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBx1Rnelzex2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pn8T89iJpIWM"
   },
   "outputs": [],
   "source": [
    "van.plot(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xruJY9IpXsA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObsqFOnc5gdR"
   },
   "source": [
    "Bootstrap-ish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XxSY_kKHzetN"
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g_1:\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "  dataset = dataset.repeat()\n",
    "  dataset = dataset.shuffle(100)\n",
    "  dataset = dataset.batch(100)\n",
    "  it = dataset.make_one_shot_iterator()\n",
    "  get_next = tf.identity(it.get_next(), name = 'next')\n",
    "  X_ =  tf.identity(it.get_next()[0], name=\"X\")\n",
    "  y_ =  tf.identity(it.get_next()[1], name=\"y\")\n",
    "\n",
    "  gdef = g_1.as_graph_def()\n",
    "  \n",
    "  #g_1.get_tensor_by_name('X:0')\n",
    "  #g_1.get_tensor_by_name(\"y:0\")\n",
    "\n",
    "  #it = dataset.make_one_shot_iterator()\n",
    "\n",
    "#X,y = it.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-EM3rMe5f-j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBNXV49V5f5M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihJ0C7Gp5fza"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qQJydif5fsX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArXS2Mi25fmh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pn4lgjcYr1xX"
   },
   "outputs": [],
   "source": [
    "plt.plot(nn.predict(X[10:50]))\n",
    "plt.plot(y[10:50])\n",
    "#plt.plot(nn.predict_meh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IrhLATk7yIym"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q3AJ_4IxeRJZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vdpPzUU0yIu_"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(12, input_dim=1, kernel_initializer='normal', activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(8, activation='tanh'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', )#metrics=['mse','mae'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "htELVxnxjNk-"
   },
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=1000, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jREmHsF_jVB8"
   },
   "outputs": [],
   "source": [
    "plt.plot(model.predict(X)[:100])\n",
    "plt.plot(y[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VE3C_-xyIrX"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "\n",
    "# tf.data.Dataset instance\n",
    "tr_data = np.random.random((1000, 32)).astype(np.float32)\n",
    "tr_label = np.random.randint(low=0, high=10, size = 1000).astype(np.int32)\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((tr_data, tr_label))\n",
    "tr_dataset = tr_dataset.batch(batch_size=32)\n",
    "tr_dataset = tr_dataset.repeat()\n",
    "\n",
    "val_data = np.random.random((100, 32)).astype(np.float32)\n",
    "val_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_label))\n",
    "val_dataset = val_dataset.batch(batch_size=100).repeat()\n",
    "\n",
    "# Training\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "model.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "model.add(keras.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(.01), \n",
    "              loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(tr_dataset, epochs = 5, steps_per_epoch = 1000 // 32,\n",
    "          validation_data = val_dataset, validation_steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJA5vvxLyIno"
   },
   "outputs": [],
   "source": [
    "# with Session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afpeWu2-yIkH"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZDPVGU-yIfS"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.shuffle(1)\n",
    "dataset = dataset.batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-MP4o6Kzlil"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0wzRNicyIa2"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(32))\n",
    "model.add(tf.keras.layers.Dense(32))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# This builds the model for the first time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQ3wSR1yyxkM"
   },
   "outputs": [],
   "source": [
    "model.fit(dataset.make_one_shot_iterator().get_next()[0],dataset.make_one_shot_iterator().get_next()[1], epochs=5, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CBsIzdwEy_Jn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzSlLT_6y_E5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vifoy2mkGjTy"
   },
   "outputs": [],
   "source": [
    "nn.fit_dataset(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aG5eORsHHEhN"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, x_in, input_size):\n",
    "        self.input_size = input_size\n",
    "        # self.x_in = tf.placeholder(dtype=tf.float32, shape=(None, self.input_size))  # Original\n",
    "        self.x_in = x_in\n",
    "        self.output_size = 3\n",
    "\n",
    "        #tf.reset_default_graph()  # This turned out to be the problem\n",
    "\n",
    "        self.layer = tf.layers.dense(self.x_in, self.output_size, activation=tf.nn.relu)\n",
    "        self.loss = tf.reduce_sum(tf.square(self.layer - tf.constant(0, dtype=tf.float32, shape=[self.output_size])))\n",
    "\n",
    "data_array = np.random.standard_normal([4, 10]).astype(np.float32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data_array).batch(2)\n",
    "\n",
    "model = Network(x_in=dataset.make_one_shot_iterator().get_next(), input_size=dataset.output_shapes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7JyPC1Ocqt-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fHqu-4jSjom"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPioZxDXSkW8"
   },
   "source": [
    "NEW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5HNs7DeSlWf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Okl_edzQSlug"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "\n",
    "# 推論\n",
    "\n",
    "def inference(x):\n",
    "    with tf.name_scope('l1') as scope:\n",
    "        w_l1 = tf.Variable(tf.truncated_normal(\n",
    "            [10, 256], stddev=0.33), name=\"w_l1\")\n",
    "        b_l1 = tf.Variable(tf.constant(1.0, shape=[256]), name=\"b_l1\")\n",
    "        h_l1 = tf.nn.relu(tf.matmul(x, w_l1) + b_l1)\n",
    "\n",
    "    with tf.name_scope('l2') as scope:\n",
    "        w_l2 = tf.Variable(tf.truncated_normal(\n",
    "            [256, 1], stddev=0.33), name=\"w_l2\")\n",
    "        b_l2 = tf.Variable(tf.constant(1.0, shape=[1]), name=\"b_l2\")\n",
    "        output = tf.nn.relu(tf.matmul(h_l1, w_l2) + b_l2)\n",
    "\n",
    "    return output\n",
    "\n",
    "# 損失関数\n",
    "def loss(model, y):\n",
    "    return tf.reduce_mean(tf.square(model - y), name=\"loss\")\n",
    "\n",
    "# 学習\n",
    "def training(loss, rate):\n",
    "    return tf.train.AdagradOptimizer(rate).minimize(loss)\n",
    "\n",
    "def main():\n",
    "    diabetes = datasets.load_diabetes()\n",
    "    data = diabetes[\"data\"].astype(np.float32)\n",
    "    target = diabetes['target'].astype(\n",
    "        np.float32).reshape(len(diabetes['target']), 1)\n",
    "\n",
    "    MAX_SIZE = data.shape[0]\n",
    "    TEST_N = 100\n",
    "    N = MAX_SIZE - TEST_N\n",
    "    BATCH_SIZE = 10\n",
    "    MAX_STEPS = 300\n",
    "\n",
    "    train_x, test_x = np.vsplit(data, [N])\n",
    "    train_y, test_y = np.vsplit(target, [N])\n",
    "\n",
    "    # symbolic variables\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "    model = inference(x)\n",
    "    loss_value = loss(model, y)\n",
    "    train_op = training(loss_value, 0.04)\n",
    "\n",
    "    best = float(\"inf\")\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        for step in range(MAX_STEPS + 1):\n",
    "            for i in range(N // BATCH_SIZE):\n",
    "                batch = BATCH_SIZE * i\n",
    "                train_batch_x = train_x[batch:batch + BATCH_SIZE]\n",
    "                train_batch_y = train_y[batch:batch + BATCH_SIZE]\n",
    "\n",
    "                loss_train = sess.run(loss_value, feed_dict={\n",
    "                                      x: train_batch_x, y: train_batch_y})\n",
    "                sess.run(train_op, feed_dict={\n",
    "                         x: train_batch_x, y: train_batch_y})\n",
    "\n",
    "            if loss_train < best:\n",
    "                best = loss_train\n",
    "                best_match = sess.run(model, feed_dict={x: test_x, y: test_y})\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                cor = np.corrcoef(best_match.flatten(), test_y.flatten())\n",
    "                print('step : {}, train loss : {}, test cor : {}'.format(\n",
    "                    step, best, cor[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUq5Rm6OSmXc"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kS7lAbHuSqCw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tf_datasets_ensemble.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
